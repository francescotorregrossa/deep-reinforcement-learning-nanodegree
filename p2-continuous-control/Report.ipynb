{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "The environment for this project is [Reacher](https://github.com/udacity/deep-reinforcement-learning/tree/master/p2_continuous_control) from Unity, and it's provided in the `setup` folder. We'll implement the A2C algorithm as the synchronous version of [A3C](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) (but the input won't be directly from pixels). Results will be shown in this notebook and the best solution will be implemented in `main.py`.\n",
    "\n",
    "![final](imgs/gif.gif \"final\")\n",
    "\n",
    "> The model used to generate this gif is `final.pth` (Dueling Double DQN), which was trained for 700 episodes using `main.py`.\n",
    "\n",
    "## 1. Prepare dependencies and environment\n",
    "\n",
    "Take a look at README.md before executing this notebook and make sure that the kernel is set to **p2_continuous_control**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install ./setup\n",
    "\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "import numpy as np\n",
    "from numpy_ringbuffer import RingBuffer\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "from setup import unityagents\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "source": [
    "Unity environments contain **brains**, our interfaces for controlling agents. We'll be conrtolling the first (default) brain in the environment. It's also useful to keep information such as `state_size`, `action_size` and `num_agents`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:unityagents:\n'Academy' started successfully!\nUnity Academy name: Academy\n        Number of Brains: 1\n        Number of External Brains : 1\n        Lesson number : 0\n        Reset Parameters :\n\t\tgoal_size -> 5.0\n\t\tgoal_speed -> 1.0\nUnity brain name: ReacherBrain\n        Number of Visual Observations (per agent): 0\n        Vector Observation space type: continuous\n        Vector Observation space size (per agent): 33\n        Number of stacked Vector Observation: 1\n        Vector Action space type: continuous\n        Vector Action space size (per agent): 4\n        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = None\n",
    "system = platform.system()\n",
    "if system == 'Linux':\n",
    "    env = UnityEnvironment(file_name=\"setup/Reacher_Linux/Reacher.x86_64\")\n",
    "elif system == 'Darwin':\n",
    "    env = UnityEnvironment(file_name=\"setup/Reacher.app\")\n",
    "elif system == 'Windows':\n",
    "    env = UnityEnvironment(file_name=\"setup/Reacher_Windows_x86_64/Reacher.exe\")\n",
    "else:\n",
    "    print('Cannot find environment for this system.')\n",
    "\n",
    "# use the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "num_agents = len(env_info.agents)\n",
    "action_size = brain.vector_action_space_size\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Actor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of agents: 20\nSize of each action: 4\nThere are 20 agents. Each observes a state with length: 33\nThe state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, hidden_layers=[64, 128, 64]):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        # prepare the first hidden layer\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(state_size, hidden_layers[0])])\n",
    "        \n",
    "        # prepare the rest of the hidden layers\n",
    "        A = hidden_layers[:-1]\n",
    "        B = hidden_layers[1:]\n",
    "        self.hidden_layers.extend([nn.Linear(a, b) for a, b in zip(A, B)])\n",
    "        \n",
    "        # prepare the output layer\n",
    "        self.output_layer = nn.Linear(hidden_layers[-1], action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        # connect layers to each other and put relu activations between them\n",
    "        for layer in self.hidden_layers:\n",
    "            state = layer(state)\n",
    "            state = F.relu(state)\n",
    "        state = self.output_layer(state)\n",
    "        return state\n",
    "    \n",
    "    def act(self, state):\n",
    "        # distrib = forward(state)\n",
    "        # sample(distrib)\n",
    "        pass\n",
    "    \n",
    "    def log_likelihood(self, state, action):\n",
    "        # distrib = forward(state)\n",
    "        # l = likelihood(distrib, action)\n",
    "        # return log(l)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Critic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total score (averaged over agents) this episode: 0.11999999731779099\n"
     ]
    }
   ],
   "source": [
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, hidden_layers=[64, 128, 64]):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        # prepare the first hidden layer\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(state_size, hidden_layers[0])])\n",
    "        \n",
    "        # prepare the rest of the hidden layers\n",
    "        A = hidden_layers[:-1]\n",
    "        B = hidden_layers[1:]\n",
    "        self.hidden_layers.extend([nn.Linear(a, b) for a, b in zip(A, B)])\n",
    "        \n",
    "        # prepare the output layer\n",
    "        self.output_layer = nn.Linear(hidden_layers[-1], action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        # connect layers to each other and put relu activations between them\n",
    "        for layer in self.hidden_layers:\n",
    "            state = layer(state)\n",
    "            state = F.relu(state)\n",
    "        state = self.output_layer(state)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advantage Estimation\n",
    "\n",
    "### 4.1 $n$-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_step():\n",
    "    pass"
   ]
  },
  {
   "source": [
    "### 4.2 GAE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gae():\n",
    "    pass"
   ]
  },
  {
   "source": [
    "## 5. Agent"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CAgent():\n",
    "    \n",
    "    # -- initialization -- #\n",
    "    def __init__(self, QNetwork, state_size, action_size, \n",
    "                 replay_buffer, Delta, \n",
    "                 eps=1, eps_decay=0.9995, min_eps=0.0001,\n",
    "                 gamma=0.99, alpha=0.001, tau=0.01,\n",
    "                 update_every=15, batch_size=64, learning=True):\n",
    "        self.state_size, self.action_size = state_size, action_size\n",
    "        self.original_eps = eps\n",
    "        self.QNetwork = QNetwork\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.Delta = Delta\n",
    "        self.learning = learning\n",
    "        self.eps, self.eps_decay, self.min_eps = eps, eps_decay, min_eps\n",
    "        self.gamma, self.alpha, self.tau = gamma, alpha, tau\n",
    "        self.update_every, self.batch_size = update_every, batch_size\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.replay_buffer.reset()\n",
    "        self.eps = self.original_eps\n",
    "        self.q_local = self.QNetwork(self.state_size, self.action_size).to(device)\n",
    "        self.q_target = self.QNetwork(self.state_size, self.action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.q_local.parameters(), lr=self.alpha)\n",
    "        self.update_i = 0\n",
    "    # -- initialization -- #\n",
    "\n",
    "    def act(self, s):\n",
    "        pass\n",
    "\n",
    "    def learn(self):\n",
    "        pass"
   ]
  },
  {
   "source": [
    "## 6. Training an agent"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = [('A2C', QNetworkAgent(DQN, state_size, action_size, UniformReplayBuffer(100_000), dt_dqn)),\n",
    "          ('A2C+GAE', QNetworkAgent(DuelingDQN, state_size, action_size, UniformReplayBuffer(100_000), dt_double_dqn))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_episode(agent, env):\n",
    "    pass\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env, episodes=1000, repeat=3, consecutive_episodes=100, show_output=True, save_as=None):\n",
    "    pass"
   ]
  },
  {
   "source": [
    "## 7. Comparing the results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 8. Possible improvements"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 9. Conclusions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p2_continuous_control",
   "language": "python",
   "name": "p2_continuous_control"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}