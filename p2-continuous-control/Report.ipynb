{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "The environment for this project is [Reacher](https://github.com/udacity/deep-reinforcement-learning/tree/master/p2_continuous_control) from Unity, and it's provided in the `setup` folder. We'll implement the A2C algorithm as the synchronous version of [A3C](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) (but the input won't be directly from pixels). Results will be shown in this notebook and the best solution will be implemented in `main.py`.\n",
    "\n",
    "![final](imgs/gif.gif \"final\")\n",
    "\n",
    "> The model used to generate this gif is `final.pth` (Dueling Double DQN), which was trained for 700 episodes using `main.py`.\n",
    "\n",
    "## 1. Prepare dependencies and environment\n",
    "\n",
    "Take a look at README.md before executing this notebook and make sure that the kernel is set to **p2_continuous_control**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install ./setup\n",
    "\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "import numpy as np\n",
    "from numpy_ringbuffer import RingBuffer\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions.normal import Normal\n",
    "from torch import nn\n",
    "\n",
    "from setup import unityagents\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "source": [
    "Unity environments contain **brains**, our interfaces for controlling agents. We'll be conrtolling the first (default) brain in the environment. It's also useful to keep information such as `state_size` and `action_size`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:unityagents:\n'Academy' started successfully!\nUnity Academy name: Academy\n        Number of Brains: 1\n        Number of External Brains : 1\n        Lesson number : 0\n        Reset Parameters :\n\t\tgoal_speed -> 1.0\n\t\tgoal_size -> 5.0\nUnity brain name: ReacherBrain\n        Number of Visual Observations (per agent): 0\n        Vector Observation space type: continuous\n        Vector Observation space size (per agent): 33\n        Number of stacked Vector Observation: 1\n        Vector Action space type: continuous\n        Vector Action space size (per agent): 4\n        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = None\n",
    "system = platform.system()\n",
    "if system == 'Linux':\n",
    "    env = UnityEnvironment(file_name=\"setup/Reacher_Linux/Reacher.x86_64\")\n",
    "elif system == 'Darwin':\n",
    "    env = UnityEnvironment(file_name=\"setup/Reacher.app\")\n",
    "elif system == 'Windows':\n",
    "    env = UnityEnvironment(file_name=\"setup/Reacher_Windows_x86_64/Reacher.exe\")\n",
    "else:\n",
    "    print('Cannot find environment for this system.')\n",
    "\n",
    "# use the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "state_size = env_info.vector_observations.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Actor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, hidden_layers=[64, 128, 64]):\n",
    "        super(Actor, self).__init__()\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # prepare the first hidden layer\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(state_size, hidden_layers[0])])\n",
    "        \n",
    "        # prepare the rest of the hidden layers\n",
    "        A = hidden_layers[:-1]\n",
    "        B = hidden_layers[1:]\n",
    "        self.hidden_layers.extend([nn.Linear(a, b) for a, b in zip(A, B)])\n",
    "        \n",
    "        # the actor will output the parameters of a normal distribution, \n",
    "        # so for each action we need mu and sigma^2 (thus we double the action size)\n",
    "        self.output_layer = nn.Linear(hidden_layers[-1], action_size * 2)\n",
    "\n",
    "    def forward(self, state):\n",
    "        # connect layers to each other and put relu activations between them\n",
    "        for layer in self.hidden_layers:\n",
    "            state = layer(state)\n",
    "            state = F.relu(state)\n",
    "        state = self.output_layer(state)\n",
    "        \n",
    "        # reshape output in two rows (mu and sigma^2)\n",
    "        distr_params = state.view(2, self.action_size)\n",
    "        \n",
    "        # mu is linear while sigma^2 uses a softplus\n",
    "        distr_params[1, :] = F.softplus(distr_params[1, :])\n",
    "\n",
    "        return distr_params\n",
    "    \n",
    "    # -- some utility functions -- #\n",
    "\n",
    "    def distribution(self, distr_params):\n",
    "        # then construct a normal distribution that depends on these values\n",
    "        distribution = Normal(distr_params[0, :], distr_params[1, :])\n",
    "        return distribution\n",
    "\n",
    "    def act(self, distribution):\n",
    "        # sample an action from a state's distribution\n",
    "        \n",
    "        # this policy is naturally stochastic, so there's no need to\n",
    "        # force randomnes using a strategy like eps-greedy\n",
    "        return distribution.sample()\n",
    "\n",
    "    def log_prob(self, distributions, actions):\n",
    "        # using a state's density function (pdf), calculate how likely it is to take the given action\n",
    "\n",
    "        # advantage (to be defined) will determine whether this value will increase or decrease\n",
    "        output = torch.empty([1, 1])\n",
    "        for distr, a in zip(distributions, actions):\n",
    "            output = torch.cat((output, distr.log_prob(a)), dim=0)\n",
    "        return output\n",
    "    \n",
    "    def entropy(self, distributions):\n",
    "        # calculate the entropy of a state's distribution (in other words, how uncertain the \n",
    "        # result is -- which only depends on sigma in the case of a normal distribution)\n",
    "\n",
    "        # this will be maximized so that some exploration is always encouraged\n",
    "        output = torch.empty([1, 1])\n",
    "        for distr in distributions:\n",
    "            output = torch.cat((output, distribution.entropy()), dim=0)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Critic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, hidden_layers=[64, 128, 64]):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        # prepare the first hidden layer\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(state_size, hidden_layers[0])])\n",
    "        \n",
    "        # prepare the rest of the hidden layers\n",
    "        A = hidden_layers[:-1]\n",
    "        B = hidden_layers[1:]\n",
    "        self.hidden_layers.extend([nn.Linear(a, b) for a, b in zip(A, B)])\n",
    "        \n",
    "        # the critic outputs only a scalar V(s)\n",
    "        self.output_layer = nn.Linear(hidden_layers[-1], 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        # connect layers to each other and put relu activations between them\n",
    "        for layer in self.hidden_layers:\n",
    "            state = layer(state)\n",
    "            state = F.relu(state)\n",
    "        state = self.output_layer(state)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advantage Estimation\n",
    "\n",
    "### 4.1 $n$-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_step():\n",
    "    pass"
   ]
  },
  {
   "source": [
    "### 4.2 GAE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gae():\n",
    "    pass"
   ]
  },
  {
   "source": [
    "## 5. Agent"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CAgent():\n",
    "    \n",
    "    # -- initialization -- #\n",
    "    def __init__(self, state_size, action_size, calc_advantage, \n",
    "                 gamma=0.99, alpha=0.001, beta=0.01, tau=0.25,\n",
    "                 n=4, learning=True):\n",
    "        self.state_size, self.action_size = state_size, action_size\n",
    "        self.calc_advantage = calc_advantage\n",
    "        self.learning = learning\n",
    "        self.gamma, self.alpha, self.tau, self.n = gamma, alpha, tau, n\n",
    "        self.reset()\n",
    "\n",
    "    def reset_temporary_buffer(self):\n",
    "        # used to store n consecutive steps\n",
    "        self.tmp_s, self.tmp_a, self.tmp_r, self.tmp_ns, self.tmp_d = \\\n",
    "            ([None] * self.n, [None] * self.n, [None] * self.n, [None] * self.n, [None] * self.n)\n",
    "        \n",
    "        # note that the critic will evaluate n+1 states\n",
    "        self.tmp_actor_out = [None] * self.n\n",
    "        self.tmp_critic_out = [None] * (self.n + 1)\n",
    "\n",
    "    \n",
    "    def reset(self):\n",
    "        self.actor = Actor(state_size, action_size).to(device)\n",
    "        self.critic = Critic(state_size).to(device)\n",
    "\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.alpha)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=self.alpha)\n",
    "        \n",
    "        self.reset_temporary_buffer()\n",
    "        self.i = 0\n",
    "    # -- initialization -- #\n",
    "\n",
    "    def act(self, s):\n",
    "        distrib = self.actor(s)\n",
    "\n",
    "        self.tmp_actor_out[self.update_i] = distrib\n",
    "\n",
    "        return self.actor.act(distrib)\n",
    "\n",
    "    def store(self, s, a, r, ns, d):\n",
    "        self.tmp_s[self.i], self.tmp_a[self.i], self.tmp_r[self.i], self.tmp_ns[self.i], self.tmp_d[self.i] = (s, a, r, ns, d)\n",
    "\n",
    "        if self.i == self.n - 1:\n",
    "            self.learn()\n",
    "            self.beta *= self.beta\n",
    "        \n",
    "        self.i = (self.i + 1) % self.n\n",
    "\n",
    "    def calc_returns(self, rewards):\n",
    "        pass\n",
    "\n",
    "    def learn(self):        \n",
    "        self.tmp_s, self.tmp_a, self.tmp_r, self.tmp_ns, self.tmp_d, self.tmp_actor_out = (\n",
    "            torch.tensor(self.tmp_s).to(device),\n",
    "            torch.tensor(self.tmp_a).to(device),\n",
    "            torch.tensor(self.tmp_r).to(device),\n",
    "            torch.tensor(self.tmp_ns).to(device),\n",
    "            torch.tensor(self.tmp_d).to(device),\n",
    "            torch.tensor(self.tmp_actor_out).to(device)\n",
    "        )\n",
    "        \n",
    "        # use the critic to evaluate all n+1 states\n",
    "        state_values = self.critic(torch.cat((self.tmp_s, self.ns[-1, :]), dim=0))\n",
    "\n",
    "        # calculate some slightly more accurate returns using n rewards and a predicion of G_n+1\n",
    "        returns = self.calc_returns(torch.cat((self.tmp_r, state_values[-1, :]), dim=0))\n",
    "\n",
    "        # calculate the advantages at each state\n",
    "        advantages = self.calc_advantage(self.tmp_r, returns, state_values, self.gamma, self.tau)\n",
    "        \n",
    "        # actor update\n",
    "        self.actor.log_prob(self.tmp_actor_out) * advantages + \\\n",
    "            self.actor.entropy(self.tmp_actor_out) * self.beta\n",
    "\n",
    "        # critic update\n",
    "        returns - state_values[:-1, :]\n",
    "\n",
    "        self.reset_temporary_buffer()\n",
    "        "
   ]
  },
  {
   "source": [
    "## 6. Training an agent"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = [('A2C, n=4', A2CAgent(state_size, action_size, n_step, n=4)),\n",
    "          ('A2C, n=8', A2CAgent(state_size, action_size, n_step, n=8)),\n",
    "          ('A2C+GAE, n=8', A2CAgent(state_size, action_size, gae, n=8))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_episode(agent, env):\n",
    "    pass\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env, episodes=1000, repeat=3, consecutive_episodes=100, show_output=True, save_as=None):\n",
    "    pass"
   ]
  },
  {
   "source": [
    "## 7. Comparing the results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 8. Possible improvements"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 9. Conclusions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p2_continuous_control",
   "language": "python",
   "name": "p2_continuous_control"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}