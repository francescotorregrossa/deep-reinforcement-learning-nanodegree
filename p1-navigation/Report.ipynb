{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install ./setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is already saved in the Workspace and can be accessed at the file path provided below.  Please run the next code cell without making any changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from setup import unityagents\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# please do not modify the line below\n",
    "env = UnityEnvironment(file_name=\"setup/Banana.app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agent while it is training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nenv_info = env.reset(train_mode=True)[brain_name] # reset the environment\\nstate = env_info.vector_observations[0]            # get the current state\\nscore = 0                                          # initialize the score\\nwhile True:\\n    action = np.random.randint(action_size)        # select an action\\n    env_info = env.step(action)[brain_name]        # send the action to the environment\\n    next_state = env_info.vector_observations[0]   # get the next state\\n    reward = env_info.rewards[0]                   # get the reward\\n    done = env_info.local_done[0]                  # see if episode has finished\\n    score += reward                                # update the score\\n    state = next_state                             # roll over the state to next time step\\n    if done:                                       # exit loop if episode finished\\n        break\\n    \\nprint(\"Score: {}\".format(score))\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agent while it is training.  However, **_after training the agent_**, you can download the saved model weights to watch the agent on your own machine! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project\n",
    "\n",
    "### 0. Dependencies\n",
    "\n",
    "Not trying to be funny with the index zero, I just forgot to add the dependencies and don't want to change the other numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "# !pip install numpy_ringbuffer\n",
    "from numpy_ringbuffer import RingBuffer\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Replay buffer\n",
    "\n",
    "We'll use an abstract class ReplayBuffer to mark the necessary methods and make two concrete implementations:\n",
    "\n",
    "- `UniformReplayBuffer`\n",
    "- `PrioritizedReplayBuffer`\n",
    "\n",
    "We'll provide one of them to the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    \"\"\"Abstract class for a replay buffer. Concrete implementations are UniformReplayBuffer and PrioritizedReplayBuffer.\"\"\"\n",
    "    \n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        \n",
    "    def sample(self, n):\n",
    "        \"\"\"Returns tuple of arrays ([S_t], [A_t], [R_t], [S_t+1], [done_t+1], [priority])\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def add(self, observation):\n",
    "        \"\"\"Add tuple of (S_t, A_t, R_t, S_t+1, done_t+1, priority)\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def size(self):\n",
    "        \"\"\"How many tuples are currently stored\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def max_priority(self):\n",
    "        pass\n",
    "    \n",
    "    def update_priority(self, observations):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformReplayBuffer():\n",
    "    \n",
    "    def __init__(self, size):\n",
    "        self.buff = RingBuffer(capacity=size, dtype=object)  # could be more specific\n",
    "    \n",
    "    def sample(self, n):\n",
    "        samples = np.random.choice(np.array(self.buff), n)\n",
    "        \n",
    "        s = torch.FloatTensor([sample[0] for sample in samples])\n",
    "        a = torch.LongTensor([sample[1] for sample in samples])\n",
    "        r = torch.FloatTensor([sample[2] for sample in samples])\n",
    "        ns = torch.FloatTensor([sample[3] for sample in samples])\n",
    "        d = torch.FloatTensor([sample[4] for sample in samples])\n",
    "        p = torch.ones(d.size()) / n\n",
    "        \n",
    "        return s, a, r, ns, d, p\n",
    "    \n",
    "    def add(self, observation):\n",
    "        s, a, r, ns, d, _ = observation\n",
    "        p = 1  # uniform priorities\n",
    "        self.buff.append((s, a, r, ns, d, p))\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buff)\n",
    "    \n",
    "    def max_priority(self):\n",
    "        return 0\n",
    "    \n",
    "    def update_priority(self, observations):\n",
    "        pass\n",
    "\n",
    "rb = UniformReplayBuffer(4)\n",
    "rb.add((np.array([1, 2, 3]), 2, 0.5, np.array([1, 2, 4]), 0, 0.4))\n",
    "rb.add((np.array([2, 3, 4]), 1, 0.5, np.array([1, 2, 4]), 0, 0.1))\n",
    "rb.add((np.array([3, 4, 5]), 0, 0.5, np.array([1, 2, 4]), 0, 1))\n",
    "rb.add((np.array([4, 5, 6]), 3, 0.5, np.array([1, 2, 4]), 1, 2))\n",
    "rb.add((np.array([5, 6, 7]), 2, 0.5, np.array([1, 2, 4]), 1, 4))\n",
    "s, a, r, ns, d, w = rb.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model\n",
    "\n",
    "Once again we have two choices:\n",
    "\n",
    "- `DQN`, the original Deep Q-Network\n",
    "- `DuelingDQN`, the Dueling Deep Q-Network\n",
    "\n",
    "Both models receive the current state as input and provide an estimate for the corresponding $Q$ values as outputs. Dueling DQN has two streams of independent layers, $V$ and $A$, respectively the value of a state (single value) and the advantage of taking an action (vector of the same size as $Q$), related by $A(s, a) = Q(s, a) - V(s)$. In comparison, DQN has only one stream that directly outputs $Q$.\n",
    "\n",
    "However, in the dueling architecture, we can't simply add $V$ and $A$ as $Q(s, a) = V(s) + A(s, a)$, because we won't be able to distinguish between them. We can, instead, write\n",
    "\n",
    "$$Q(s, a) = V(s) + A(s, a) - \\max_{a'} A(S, a')$$\n",
    "\n",
    "so that when the best action $a^*$ is selected, we obtain $Q(s, a^*) = V(s)$, something that is not guaranteed otherwies. To further improve this, we can substitute the $\\max$ operator for a mean of the available actions. This is the final equation used in the following model, where $|\\mathbb{A}(s)| = 4$.\n",
    "\n",
    "$$Q(s, a) = V(s) + A(s, a) - \\frac{1}{|\\mathbb{A}(s)|} \\sum_{a' \\in \\mathbb{A}(s)} A(s, a')$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, hidden_layers=[64, 128, 64]):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(state_size, hidden_layers[0])])\n",
    "        \n",
    "        A = hidden_layers[:-1]\n",
    "        B = hidden_layers[1:]\n",
    "        self.hidden_layers.extend([nn.Linear(a, b) for a, b in zip(A, B)])\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_layers[-1], action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        for layer in self.hidden_layers:\n",
    "            state = layer(state)\n",
    "            state = F.relu(state)\n",
    "        state = self.output_layer(state)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Error estimation\n",
    "\n",
    "The original paper for DQN uses two networks, one `local` and one as a fixed `target`, which changes less frequently. The Temporal-Difference error $\\delta_t$ is estimated as\n",
    "\n",
    "$$\\delta_t(s, a, r, s') = r + \\gamma \\max_{a'} Q_{target}(s', a') - Q_{local}(s, a)$$\n",
    "\n",
    "However, this can be improved using the Double DQN estimation. Rewriting the $\\max$ value using $\\arg \\max$, we can see that we could choose the action using the `local` network and evaluate it with the `target` network. This will hopefully decrease noise, as both networks have to \"agree\" on the outcome of actions.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\max_{a'} Q_{target}(s', a') = & \\ Q_{target}(s', \\arg \\max_{a'} Q_{target}(s', a'))\\\\\n",
    "& \\ Q_{target}(s', \\arg \\max_{a'} Q_{local}(s', a'))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The final equation is:\n",
    "\n",
    "$$\\delta_t(s, a, r, s') = r + \\gamma Q_{target}(s', \\arg \\max_{a'} Q_{local}(s', a')) - Q_{local}(s, a)$$\n",
    "\n",
    "The agent will rely on one of the two functions, `dt_dqn`, `dt_double_dqn`. Both these functions take an extra boolean parameter `d` that signals when the episode is over. In that case, we should only consider the immediate reward, so the TD error simplifies to $\\delta_t = r - Q_{local}(s, a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt_dqn(s, a, r, ns, d, q_local, q_target, gamma):\n",
    "    \"\"\"\n",
    "    QL [5 x 4]\n",
    "               a_0      a_1       a_2     a_3\n",
    "    tensor([[ 0.0060, -0.0931,  0.0496, -0.0626],    # 0 in the batch\n",
    "            [ 0.0139, -0.0952,  0.0470, -0.0571],    # 1 \n",
    "            [ 0.0024, -0.0916,  0.0288, -0.0674],    # 2\n",
    "            [-0.0207, -0.1126,  0.0453, -0.1079],    # 3\n",
    "            [ 0.0089, -0.1415,  0.0422, -0.1021]])   # 4\n",
    "    \n",
    "    a [5] \n",
    "    tensor([1, 3, 1, 0, 2])\n",
    "    \n",
    "    a.unsqueeze(1) [5 x 1]\n",
    "    tensor([[ 1],\n",
    "            [ 3],\n",
    "            [ 1],\n",
    "            [ 0],\n",
    "            [ 2]])\n",
    "    \n",
    "    QL.gather(1, a.unsqueeze(1)) [5 x 1]\n",
    "    tensor(1.00000e-02 *\n",
    "           [[-9.3088],\n",
    "            [-5.7069],\n",
    "            [-9.1603],\n",
    "            [-2.0720],\n",
    "            [ 4.2235]])\n",
    "    \n",
    "    QL.squeeze [5]\n",
    "    tensor([-0.0931, -0.0571, -0.0916, -0.0207, 0.0422])\n",
    "    \"\"\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        QT = q_target(ns).max(1)[0]\n",
    "    QL = q_local(s).gather(1, a.unsqueeze(1)).squeeze(1)    \n",
    "    return r + gamma * QT * (1 - torch.FloatTensor(d)) - QL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt_double_dqn(s, a, r, ns, d, q_local, q_target, gamma):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Agent\n",
    "\n",
    "Let's put the pieces together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetworkAgent():\n",
    "    \n",
    "    def __init__(self, QNetwork, state_size, action_size, \n",
    "                 replay_buffer, Delta, \n",
    "                 eps=1, eps_decay=0.9995, min_eps=0.0001, gamma=0.99, \n",
    "                 alpha=0.001, tau=0.01,\n",
    "                 update_every=4, batch_size=64, learning=True):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.q_local = QNetwork(state_size, action_size)\n",
    "        self.q_target = QNetwork(state_size, action_size)\n",
    "        self.q_target.eval()\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.Delta = Delta\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.q_local.parameters(), lr=alpha)\n",
    "        \n",
    "        self.eps = eps\n",
    "        self.eps_decay = eps_decay\n",
    "        self.min_eps = min_eps\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.tau = tau\n",
    "        \n",
    "        self.update_every = update_every\n",
    "        self.update_i = 0\n",
    "        \n",
    "        self.learning = learning\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def set_learning(self, learning=True):\n",
    "        self.learning = learning\n",
    "        \n",
    "    def act(self, s):\n",
    "        if not self.learning or np.random.uniform() > self.eps:\n",
    "            with torch.no_grad():\n",
    "                s = torch.FloatTensor(s).unsqueeze(0)\n",
    "                return int(self.q_local(s).max(1)[1])\n",
    "        else:\n",
    "            return np.random.randint(self.action_size)\n",
    "    \n",
    "    def store(self, s, a, r, ns, d):\n",
    "        p = self.replay_buffer.max_priority()\n",
    "        self.replay_buffer.add((s, a, r, ns, d, p))\n",
    "        if self.update_i == 0 and self.replay_buffer.size() >= self.batch_size:\n",
    "            self.learn()\n",
    "        self.update_i = (self.update_i + 1) % self.update_every\n",
    "        self.eps = max(self.eps * self.eps_decay, self.min_eps)\n",
    "    \n",
    "    def learn(self):\n",
    "        s, a, r, ns, d, w = self.replay_buffer.sample(self.batch_size)\n",
    "        td_delta = self.Delta(s, a, r, ns, d, self.q_local, self.q_target, self.gamma)\n",
    "        self.replay_buffer.update_priority(zip(s, a, r, ns, d, torch.abs(td_delta)))\n",
    "                \n",
    "        self.optimizer.zero_grad()\n",
    "        loss = torch.sum(w * (td_delta ** 2))  # weighted mse\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for local, target in zip(self.q_local.parameters(), self.q_target.parameters()):\n",
    "                target.copy_(target + self.tau * (local - target))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QNetworkAgent(\n",
    "    DQN,\n",
    "    state_size, action_size,\n",
    "    UniformReplayBuffer(100_000),\n",
    "    dt_dqn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100, Score: 16.0, Average: 5.59484848484843557"
     ]
    }
   ],
   "source": [
    "last100 = []\n",
    "for i in range(1, 101):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = env_info.vector_observations[0]\n",
    "    score = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "\n",
    "        env_info = env.step(action)[brain_name]\n",
    "        next_state = env_info.vector_observations[0]\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "\n",
    "        agent.store(state, action, reward, next_state, done)\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "    # if i % 10 == 0:\n",
    "    \n",
    "    last100.append(score)\n",
    "    if len(last100) > 100:\n",
    "        last100.pop(0)\n",
    "    \n",
    "    average = np.mean(last100)\n",
    "    print(\"\\rEpisode: {}, Score: {}, Average: {}\".format(i, score, average), end=\"\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    if average > 13:\n",
    "        print(\"\\rSolved in {} episodes. Average: {}\".format(i, average), end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        break\n",
    "    \n",
    "# print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(agent.q_local.state_dict(), 'checkpoint.pth')\n",
    "# agent.q_local.load_state_dict(torch.load('checkpoint.pth', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "agent.set_learning(False)\n",
    "state = env_info.vector_observations[0]\n",
    "score = 0\n",
    "done = False\n",
    "while not done:\n",
    "    action = agent.act(state)\n",
    "\n",
    "    env_info = env.step(action)[brain_name]\n",
    "    next_state = env_info.vector_observations[0]\n",
    "    reward = env_info.rewards[0]\n",
    "    done = env_info.local_done[0]\n",
    "\n",
    "    agent.store(state, action, reward, next_state, done)\n",
    "    score += reward\n",
    "    state = next_state\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p1_navigation",
   "language": "python",
   "name": "p1_navigation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
