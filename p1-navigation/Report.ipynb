{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Prepare dependencies and environment\n",
    "\n",
    "Take a look at README.md before executing this notebook and make sure that the kernel is set to **p1_navigation**. The environment for this project is [Banana](https://github.com/kotogasy/unity-ml-banana) from Unity, and it's provided in the `setup` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install ./setup\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from numpy_ringbuffer import RingBuffer\n",
    "from scipy import signal\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "from setup import unityagents\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unity environments contain **brains**, our interfaces for controlling agents. We'll be conrtolling the first (default) brain in the environment. It's also useful to keep information such as `state_size` and `action_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:unityagents:\n'Academy' started successfully!\nUnity Academy name: Academy\n        Number of Brains: 1\n        Number of External Brains : 1\n        Lesson number : 0\n        Reset Parameters :\n\t\t\nUnity brain name: BananaBrain\n        Number of Visual Observations (per agent): 0\n        Vector Observation space type: continuous\n        Vector Observation space size (per agent): 37\n        Number of stacked Vector Observation: 1\n        Vector Action space type: discrete\n        Vector Action space size (per agent): 4\n        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"setup/Banana.app\")\n",
    "\n",
    "# use the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "state = env_info.vector_observations[0]\n",
    "state_size = len(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Replay buffer\n",
    "\n",
    "We'll use the **uniform** variation of the replay buffer here, meaning that all stored tuples have the same chance of being selected for replay. Nonetheless, we can provide some additional methods and data, such as `max_priority()` and `update_priority()`, that will help us write a more abstract agent, in case we want to try the prioritized variant.\n",
    "\n",
    "Every tuple is stored as `(s, a, r, ns, d, p)` where:\n",
    "\n",
    "- `s` is the state at the beginning of the timestep\n",
    "- `a` is the action that was taken\n",
    "- `r` is the reward obtained in the next timestep\n",
    "- `ns` is the state at the next timestep (we'll refer to this as $s'$ as well)\n",
    "- `d` is a boolean value that determines if the episode ended\n",
    "- `p` is the **priority** of the tuple, ignored in this case and kept here because it will make the transition to the prioritized version easier. At the moment, however, every tuple will be considered with `p = max_priority() = 0` (not stored), _regardless of p_.\n",
    "\n",
    "When sampling a batch of `n` tuples, we'll obtain a single tuple `([s], [a], [r], [ns], [d], [w])` where:\n",
    "- `[s]`, `[a]`, `[r]`, `[ns]`, `[d]` are **torch tensors** with `n` rows. Keep in mind that `[s]` and `[ns]` can have more than one column, depending on the `state_size`.\n",
    "- `[w]` is the `n`-rows tensor of **importance sampling weights**. Since this is the uniform case, every tuple has the weight `1/n`. Again, this is done because it will make the transition to the prioritized version easier.\n",
    "\n",
    "# todo capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformReplayBuffer():\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.buff = RingBuffer(capacity=self.capacity, dtype=object)\n",
    "    \n",
    "    def sample(self, n, replace=True):\n",
    "        samples = np.random.choice(np.array(self.buff), n, replace)\n",
    "        \n",
    "        s = torch.FloatTensor([sample[0] for sample in samples])\n",
    "        a = torch.LongTensor([sample[1] for sample in samples])\n",
    "        r = torch.FloatTensor([sample[2] for sample in samples])\n",
    "        ns = torch.FloatTensor([sample[3] for sample in samples])\n",
    "        d = torch.FloatTensor([sample[4] for sample in samples])\n",
    "        w = torch.ones(d.size()) / n\n",
    "        \n",
    "        return s, a, r, ns, d, w\n",
    "    \n",
    "    def add(self, observation):\n",
    "        s, a, r, ns, d, _ = observation\n",
    "        self.buff.append((s, a, r, ns, d))\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buff)\n",
    "    \n",
    "    def max_priority(self):\n",
    "        return 0\n",
    "    \n",
    "    def update_priority(self, observations):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Q Network\n",
    "\n",
    "We have two choices for this network's architecture:\n",
    "\n",
    "- `DQN`, the original Deep Q-Network\n",
    "- `DuelingDQN`, the Dueling Deep Q-Network\n",
    "\n",
    "Both models receive the current state $s$ as input and provide a vector for all estimated $Q(s, a)$ values at once.\n",
    "\n",
    "### 3.1 DQN\n",
    "\n",
    "The `DQN` class is straightforward and uses less parameters, since it only uses one stream of fully connected layers:\n",
    "\n",
    "- `state_size` $\\to 64$ followed by `relu` activations\n",
    "- $64 \\to 128$ and `relu`\n",
    "- $128 \\to 64$ and `relu`\n",
    "- $64 \\to Q(s, a)$, that is $64 \\to$ `action_size` and no activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, hidden_layers=[64, 128, 64]):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(state_size, hidden_layers[0])])\n",
    "        \n",
    "        A = hidden_layers[:-1]  # -> [64, 128] (using the predefined values)\n",
    "        B = hidden_layers[1:]   # -> [128, 64]\n",
    "        in_out = zip(A, B)      # -> [(64, 128), (128, 64)]\n",
    "        self.hidden_layers.extend([nn.Linear(a, b) for a, b in in_out])\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_layers[-1], action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        for layer in self.hidden_layers:\n",
    "            state = layer(state)\n",
    "            state = F.relu(state)\n",
    "        state = self.output_layer(state)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 DuelingDQN\n",
    "\n",
    "On the other hand, `DuelingDQN` has two streams of independent layers: the **value** of a state $V(s)$ (scalar) and the **advantage** of taking an action $A(s, a)$ (same shape as the output), related by $A(s, a) = Q(s, a) - V(s)$. The idea is that, in many scenarios, the value of a state won't depend that much on the action the agent takes, so decoupling these values can help make better estimates.\n",
    "\n",
    "So, the class `DuelingDQN` uses two identical streams of independent fully connected layers:\n",
    "\n",
    "- `state_size` $\\to 64$ and `relu`\n",
    "- $64 \\to 128$ and `relu`\n",
    "- $128 \\to 64$ and `relu`\n",
    "\n",
    "The streams end in:\n",
    "\n",
    "- $64 \\to A(s, a)$, that is $64 \\to$ `action_size` and no activation\n",
    "- $64 \\to V(s)$, that is $64 \\to 1$ and no activation\n",
    "\n",
    "To merge them, however, we can't simply add $V$ and $A$ as $Q(s, a) = V(s) + A(s, a)$, because $A$ and $V$ **won't be identifiable**. We can, instead, write\n",
    "\n",
    "$$Q(s, a) = V(s) + A(s, a) - \\max_{a'} A(S, a')$$\n",
    "\n",
    "so that when the best action $a^*$ is selected, we get that $Q(s, a^*) = V(s)$, something that is not guaranteed otherwise. To further improve this, we can substitute the $\\max$ operator for a mean of the available actions. This is the final equation used in the `DuelingDQN` class, where $|\\mathbb{A}(s)| =$ `action_size`.\n",
    "\n",
    "$$Q(s, a) = V(s) + A(s, a) - \\frac{1}{|\\mathbb{A}(s)|} \\sum_{a' \\in \\mathbb{A}(s)} A(s, a')$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, hidden_layers=[32, 128, 32]):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        \n",
    "        self.value_hidden_layers = nn.ModuleList([nn.Linear(state_size, hidden_layers[0])])\n",
    "        self.advantage_hidden_layers = nn.ModuleList([nn.Linear(state_size, hidden_layers[0])])\n",
    "        \n",
    "        A = hidden_layers[:-1]  # -> [32, 128] (using the predefined values)\n",
    "        B = hidden_layers[1:]   # -> [128, 32]\n",
    "        in_out = zip(A, B)      # -> [(32, 128), (128, 32)]\n",
    "        self.value_hidden_layers.extend([nn.Linear(a, b) for a, b in in_out])\n",
    "        self.advantage_hidden_layers.extend([nn.Linear(a, b) for a, b in in_out])\n",
    "        \n",
    "        self.value_output_layer = nn.Linear(hidden_layers[-1], 1)\n",
    "        self.advantage_output_layer = nn.Linear(hidden_layers[-1], action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        V = x\n",
    "        A = x.clone()\n",
    "        for value_layer, advantage_layer in zip(self.value_hidden_layers, self.advantage_hidden_layers):\n",
    "            V, A = value_layer(V), advantage_layer(A)\n",
    "            V, A = F.relu(V), F.relu(A)\n",
    "        V, A = self.value_output_layer(V), self.advantage_output_layer(A)\n",
    "        return V + A - torch.mean(A, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Temporal-Difference error estimation\n",
    "\n",
    "Since this is a bootstrapping method, when we're calculating the **temporal-difference** error $\\delta_t$ we'll have to rely on an estimate of future returns. Two proposed variants for this are fixed Q-targets and Double DQN.\n",
    "\n",
    "The functions `dt_dqn`, `dt_double_dqn` implement these two variants. Both take an extra boolean parameter `d` that signals when the episode is over, since in that case, there is no future return and the TD error is simply $\\delta_t = r - Q(s, a)$.\n",
    "\n",
    "Note that these functions expect `s`, `a`, `r`, `ns` and `d` as **torch tensors**, where the number of rows indicates the batch size. The return value is also a vector with the same number of rows, one for each tuple.\n",
    "\n",
    "#### 4.1 Fixed Q-targets\n",
    "\n",
    "The original paper proposed the method of fixed Q-targets, essentially **duplicating the network**, creating $Q_{local}$ and $Q_{target}$, where the former is used to determine the policy and the latter is updated less frequently and only used to estimate future returns. In particular, the equation for $\\delta_t$ is\n",
    "\n",
    "$$\\delta_t(s, a, r, s') = r + \\gamma \\max_{a'} Q_{target}(s', a') - Q_{local}(s, a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt_dqn(s, a, r, ns, d, q_local, q_target, gamma):\n",
    "    with torch.no_grad():   # no need for gradients when we're evaluating the TD target\n",
    "        QT = q_target(ns)   # evaluate the next state using the target network (out: [n * action_size])\n",
    "        QT = QT.max(1)      # take the max along the column (actions) (out: two tensors [n])\n",
    "        QT = QT[0]          # - [0] has the max values for each element in the batch, \n",
    "                            # - [1] has the indexes of the max values\n",
    "\n",
    "    a = a.unsqueeze(1)      # reshape [n] -> [n * 1]\n",
    "    QL = q_local(s)         # evaluate the current state using the local network (out: [n * action_size])\n",
    "    QL = QL.gather(1, a)    # for each row, take the column in QL indicated by a (out: [n * 1])\n",
    "    QL = QL.squeeze(1)      # reshape [n * 1] -> [n]\n",
    "\n",
    "    return r + gamma * QT * (1 - torch.FloatTensor(d)) - QL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Double DQN\n",
    "\n",
    "This is largely based on fixed Q-targets, since it also uses $Q_{local}$ and $Q_{target}$. We can rewrite $\\max Q$ using $\\arg \\max$ to break down the task of **choosing** and **evaluating** an action in two different steps.\n",
    "\n",
    "$$\\max_{a'} Q_{target}(s', a') = Q_{target}(s', \\arg \\max_{a'} Q_{target}(s', a'))$$\n",
    "\n",
    "This can lead to the **overestimation** of targets. Simply put, when taking the $\\max$ among noisy numbers (which is especially true in the beginning) we're likely to pick the action where the approximator adds more positive noise. Separating the task of picking and evaluating the action to two different approximators may decrease this issue, since both networks now have to \"agree\" on the outcome of an action.\n",
    "\n",
    "The original estimate can be improved using the one provided in the Double DQN paper. The proposed solution is to use $Q_{local}$ for the choice and $Q_{target}$ for the evaluation as follows\n",
    "\n",
    "$$\n",
    "Q_{target}(s', \\arg \\max_{a'} Q_{local}(s', a'))\\\\\n",
    "\\implies \\delta_t(s, a, r, s') = r + \\gamma Q_{target}(s', \\arg \\max_{a'} Q_{local}(s', a')) - Q_{local}(s, a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt_double_dqn(s, a, r, ns, d, q_local, q_target, gamma):\n",
    "    with torch.no_grad():         # no need for gradients when we're evaluating the TD target\n",
    "        QLns = q_local(ns)        # evaluate the next state using the local network (out: [n * action_size])\n",
    "        QLns = QLns.max(1)        # take the max along the column (actions) (out: two tensors [n])\n",
    "        QLns = QLns[1]            # [1] has the indexes of the max values\n",
    "        QLns = QLns.unsqueeze(1)  # reshape [n] -> [n * 1]\n",
    "\n",
    "        QT = q_target(ns)         # evaluate the next state using the target network (out: [n * action_size])\n",
    "        QT = QT.gather(1, QLns)   # for each row, take the value estimated by the target network for\n",
    "                                  # the best action estimated by the local network (out: [n * 1])\n",
    "        QT = QT.squeeze(1)        # reshape [n * 1] -> [n]\n",
    "\n",
    "    a = a.unsqueeze(1)            # reshape [n] -> [n * 1]\n",
    "    QL = q_local(s)               # evaluate the current state using the local network (out: [n * action_size])\n",
    "    QL = QL.gather(1, a)          # for each row, take the column in QL indicated by a (out: [n * 1])\n",
    "    QL = QL.squeeze(1)            # reshape [n * 1] -> [n]\n",
    "\n",
    "    return r + gamma * QT * (1 - torch.FloatTensor(d)) - QL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Agent\n",
    "\n",
    "Let's put the pieces together. During initialization, we can configure the agent's:\n",
    "\n",
    "- `QNetwork`, which is either the class `DQN` or `DuelingDQN` (_not an instance_)\n",
    "- `replay_buffer`, which is an instance of `UniformReplayBuffer` (could be replaced with a `PrioritizedReplayBuffer`)\n",
    "- `Delta, which` is either the function `dt_dqn` or `dt_double_dqn`\n",
    "\n",
    "Other parameters are obviously `state_size` and `action_size` as well as some hyperparameters:\n",
    "\n",
    "- `alpha` $\\in [0, 1]$, the learning rate to apply to gradient steps to `q_local`\n",
    "- `eps`, `eps_decay` and `min_eps` $\\in [0, 1]$, for the $\\epsilon$-greedy policy, if `learning = False` only the greedy policy will be used\n",
    "- `gamma` $\\in [0, 1]$, the weight of the estimates for future returns calculated by `q_target` in the function `Delta`\n",
    "- `tau` $\\in [0, 1]$, to perform soft updates of `q_target`, if set to $1$ directly copies `q_local` into `q_target`\n",
    "- `update_every` the number of steps to wait before updating `q_local` and `q_target`\n",
    "- `batch_size` the number of samples from the `replay_buffer` used to perform one update of `q_local`\n",
    "\n",
    "The agent has three functions, but only the first two should be called from the outside: \n",
    "- `act`, based on a state tensor `s` and on `eps`, choose an action. If the agent has `learning = False` the choice will be greedy.\n",
    "- `store`, receive a tuple to put in the `replay_buffer`. This counts as a step towards the updates of `q_local` and `q_target`, and in fact calls `learn()` automatically when needed.\n",
    "- `learn`, samples a batch of tuples from the `replay_buffer`, calculates their $\\delta_t$ and finally performs gradient descent on `q_local` and a soft update of `q_target` towards `q_local`. Note that it can already perform actions related to a tuple's priority and importance sampling weight.\n",
    "\n",
    "Additionally, an agent can be `reset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetworkAgent():\n",
    "    \n",
    "    def __init__(self, QNetwork, state_size, action_size, \n",
    "                 replay_buffer, Delta, \n",
    "                 eps=1, eps_decay=0.9995, min_eps=0.0001, gamma=0.99, \n",
    "                 alpha=0.001, tau=0.01,\n",
    "                 update_every=4, batch_size=64, learning=True):\n",
    "        self.state_size, self.action_size = state_size, action_size\n",
    "        self.original_eps = eps\n",
    "        self.QNetwork = QNetwork\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.Delta = Delta\n",
    "        self.learning = learning\n",
    "        self.eps, self.eps_decay, self.min_eps = eps, eps_decay, min_eps\n",
    "        self.gamma, self.alpha, self.tau = gamma, alpha, tau\n",
    "        self.update_every, self.batch_size = update_every, batch_size\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.replay_buffer.reset()\n",
    "        self.eps = self.original_eps\n",
    "        self.q_local, self.q_target = self.QNetwork(state_size, action_size), self.QNetwork(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.q_local.parameters(), lr=self.alpha)\n",
    "        self.update_i = 0\n",
    "\n",
    "    def act(self, s):\n",
    "        if not self.learning or np.random.uniform() > self.eps:\n",
    "            with torch.no_grad():\n",
    "                s = torch.FloatTensor(s).unsqueeze(0)  # out: [1 * action_size]\n",
    "                return int(self.q_local(s).max(1)[1])\n",
    "        else:\n",
    "            return np.random.randint(self.action_size)\n",
    "    \n",
    "    def store(self, s, a, r, ns, d):\n",
    "        p = self.replay_buffer.max_priority()\n",
    "        self.replay_buffer.add((s, a, r, ns, d, p))\n",
    "        if self.update_i == 0 and self.replay_buffer.size() >= self.batch_size:\n",
    "            self.learn()\n",
    "        self.update_i = (self.update_i + 1) % self.update_every\n",
    "        self.eps = max(self.eps * self.eps_decay, self.min_eps)\n",
    "    \n",
    "    def learn(self):\n",
    "        s, a, r, ns, d, w = self.replay_buffer.sample(self.batch_size)\n",
    "        td_delta = self.Delta(s, a, r, ns, d, self.q_local, self.q_target, self.gamma)\n",
    "        self.replay_buffer.update_priority(zip(s, a, r, ns, d, torch.abs(td_delta)))\n",
    "                \n",
    "        self.optimizer.zero_grad()\n",
    "        loss = torch.sum(w * (td_delta ** 2))  # weighted mse\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():  # soft updates of q_target don't need autograd, since they're copied (or interpolated) from q_local\n",
    "            for local, target in zip(self.q_local.parameters(), self.q_target.parameters()):\n",
    "                target.copy_(target + self.tau * (local - target))"
   ]
  },
  {
   "source": [
    "To create an agent, we really just need a few lines now."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QNetworkAgent(\n",
    "    DuelingDQN,\n",
    "    state_size, action_size,\n",
    "    UniformReplayBuffer(100_000),\n",
    "    dt_double_dqn\n",
    ")"
   ]
  },
  {
   "source": [
    "## 6. Training an agent\n",
    "\n",
    "What we need now is a function that connects our `agent` and `env`. Recall that we'll be using the default `brain_name` to interact with this environment.\n",
    "\n",
    "Also, we should **monitor** what happens so that we'll be able to compare different approaches. Our most important indicator is going to be the average score over 100 consecutive episodes: the environment is considered **solved when the agent scores +13 or more** under this metric. To reduce noise in the output, we'll be training the agent 10 times from scratch, for 1000 episodes, and we'll average these results.\n",
    "\n",
    "After we compare the results and find out which strategy performs best, we can train an agent of that type with `repeat=1` for reasonable number of episodes, and we're done."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env, episodes=1000, repeat=10, consecutive_episodes=100, show_output=True):\n",
    "\n",
    "    results = [None] * repeat\n",
    "    for r in range(repeat):\n",
    "        \n",
    "        agent.reset()\n",
    "\n",
    "        partial_results = [None] * episodes\n",
    "        for i in range(episodes):\n",
    "            \n",
    "            env_info = env.reset(train_mode=True)[brain_name]\n",
    "            state = env_info.vector_observations[0]\n",
    "            score = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = agent.act(state)\n",
    "\n",
    "                env_info = env.step(action)[brain_name]\n",
    "                next_state = env_info.vector_observations[0]\n",
    "                reward = env_info.rewards[0]\n",
    "                done = env_info.local_done[0]\n",
    "\n",
    "                agent.store(state, action, reward, next_state, done)\n",
    "                score += reward\n",
    "                state = next_state\n",
    "\n",
    "            partial_results[i] = score\n",
    "            \n",
    "            if show_output:\n",
    "                print(\"\\r[{}] Episode: {}, Score: {}\".format(r+1, i+1, score), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "        \n",
    "        results[r] = partial_results\n",
    "    \n",
    "    # use convolutions to calculate the mean and standard deviation summarizing the training step\n",
    "    results = np.array(results)\n",
    "    mean = signal.convolve2d(results, np.ones([repeat, consecutive_episodes]) / (repeat * consecutive_episodes), mode='valid')    \n",
    "    v = signal.convolve2d(results, np.ones([1, consecutive_episodes]) / consecutive_episodes, mode='valid')\n",
    "    std_dev = signal.convolve2d(v ** 2 - mean ** 2, np.ones([repeat, 1]) / repeat, mode='valid') ** (1/2)\n",
    "    return mean.flatten(), std_dev.flatten(), results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[3] Episode: 600, Score: 15.0"
     ]
    }
   ],
   "source": [
    "agents = [QNetworkAgent(DQN, state_size, action_size, UniformReplayBuffer(100_000), dt_dqn),\n",
    "          QNetworkAgent(DQN, state_size, action_size, UniformReplayBuffer(100_000), dt_double_dqn),\n",
    "          QNetworkAgent(DuelingDQN, state_size, action_size, UniformReplayBuffer(100_000), dt_dqn),\n",
    "          QNetworkAgent(DuelingDQN, state_size, action_size, UniformReplayBuffer(100_000), dt_double_dqn)]\n",
    "\n",
    "training_results = []\n",
    "for agent in agents:\n",
    "    mean, std_dev, _ = train(agent, env, repeat=3, episodes=600)\n",
    "    training_results.append((mean, std_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(agent.q_local.state_dict(), 'checkpoint.pth')\n",
    "# agent.q_local.load_state_dict(torch.load('checkpoint.pth', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "13.406666666666661 435\n14.223333333333322 493\n14.973333333333315 476\n14.933333333333332 354\n"
     ]
    }
   ],
   "source": [
    "for ts in training_results:\n",
    "    mean, std_dev = ts\n",
    "    print(np.max(mean), np.argmax(mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7.0\n"
     ]
    }
   ],
   "source": [
    "agent = agents[-1]\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "agent.learning = False\n",
    "state = env_info.vector_observations[0]\n",
    "score = 0\n",
    "done = False\n",
    "while not done:\n",
    "    action = agent.act(state)\n",
    "\n",
    "    env_info = env.step(action)[brain_name]\n",
    "    next_state = env_info.vector_observations[0]\n",
    "    reward = env_info.rewards[0]\n",
    "    done = env_info.local_done[0]\n",
    "\n",
    "    agent.store(state, action, reward, next_state, done)\n",
    "    score += reward\n",
    "    state = next_state\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p1_navigation",
   "language": "python",
   "name": "p1_navigation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}