{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Prepare dependencies and environment\n",
    "\n",
    "Take a look at README.md before executing this notebook and make sure that the kernel is set to **p1_navigation**. The environment for this project is [Banana](https://github.com/kotogasy/unity-ml-banana) from Unity, and it's provided in the `setup` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install ./setup\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from numpy_ringbuffer import RingBuffer\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "from setup import unityagents\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unity environments contain **brains**, our interfaces for controlling agents. We'll be conrtolling the first (default) brain in the environment. It's also useful to keep information such as `state_size` and `action_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:unityagents:\n'Academy' started successfully!\nUnity Academy name: Academy\n        Number of Brains: 1\n        Number of External Brains : 1\n        Lesson number : 0\n        Reset Parameters :\n\t\t\nUnity brain name: BananaBrain\n        Number of Visual Observations (per agent): 0\n        Vector Observation space type: continuous\n        Vector Observation space size (per agent): 37\n        Number of stacked Vector Observation: 1\n        Vector Action space type: discrete\n        Vector Action space size (per agent): 4\n        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"setup/Banana.app\")\n",
    "\n",
    "# use the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "state = env_info.vector_observations[0]\n",
    "state_size = len(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Replay buffer\n",
    "\n",
    "We'll use the **uniform** variation of the replay buffer here, meaning that all stored tuples have the same chance of being selected for replay. Nonetheless, we can provide some additional methods and data, such as `max_priority()` and `update_priority()`, that will help us write a more abstract agent, in case we want to try the prioritized variant.\n",
    "\n",
    "Every tuple is stored as `(s, a, r, ns, d, p)` where:\n",
    "\n",
    "- `s` is the state at the beginning of the timestep\n",
    "- `a` is the action that was taken\n",
    "- `r` is the reward obtained in the next timestep\n",
    "- `ns` is the state at the next timestep (we'll refer to this as $s'$ as well)\n",
    "- `d` is a boolean value that determines if the episode ended\n",
    "- `p` is the **priority** of the tuple, ignored in this case and kept here because it will make the transition to the prioritized version easier. At the moment, however, every tuple will be considered with `p = max_priority() = 0` (not stored), _regardless of p_.\n",
    "\n",
    "When sampling a batch of `n` tuples, we'll obtain a single tuple `([s], [a], [r], [ns], [d], [w])` where:\n",
    "- `[s]`, `[a]`, `[r]`, `[ns]`, `[d]` are **torch tensors** with `n` rows. Keep in mind that `[s]` and `[ns]` can have more than one column, depending on the `state_size`.\n",
    "- `[w]` is the `n`-rows tensor of **importance sampling weights**. Since this is the uniform case, every tuple has the weight `1/n`. Again, this is done because it will make the transition to the prioritized version easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code is commented in ReplayBuffers/UniformReplayBuffer.py\n",
    "class UniformReplayBuffer():\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.buff = RingBuffer(capacity=size, dtype=object)\n",
    "    \n",
    "    def sample(self, n, replace=True):\n",
    "        samples = np.random.choice(np.array(self.buff), n, replace)\n",
    "        \n",
    "        s = torch.FloatTensor([sample[0] for sample in samples])\n",
    "        a = torch.LongTensor([sample[1] for sample in samples])\n",
    "        r = torch.FloatTensor([sample[2] for sample in samples])\n",
    "        ns = torch.FloatTensor([sample[3] for sample in samples])\n",
    "        d = torch.FloatTensor([sample[4] for sample in samples])\n",
    "        w = torch.ones(d.size()) / n\n",
    "        \n",
    "        return s, a, r, ns, d, w\n",
    "    \n",
    "    def add(self, observation):\n",
    "        s, a, r, ns, d, _ = observation\n",
    "        self.buff.append((s, a, r, ns, d))\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buff)\n",
    "    \n",
    "    def max_priority(self):\n",
    "        return 0\n",
    "    \n",
    "    def update_priority(self, observations):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Q Network\n",
    "\n",
    "We have two choices for this network's architecture:\n",
    "\n",
    "- `DQN`, the original Deep Q-Network\n",
    "- `DuelingDQN`, the Dueling Deep Q-Network\n",
    "\n",
    "Both models receive the current state $s$ as input and provide a vector for all estimated $Q(s, a)$ values at once.\n",
    "\n",
    "#### 3.1 DQN\n",
    "\n",
    "The `DQN` class is straightforward and uses less parameters, since it only uses one stream of fully connected layers:\n",
    "\n",
    "- `state_size` $\\to 64$ followed by `relu` activations\n",
    "- $64 \\to 128$ and `relu`\n",
    "- $128 \\to 64$ and `relu`\n",
    "- $64 \\to Q(s, a)$, that is $64 \\to$ `action_size` and no activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, hidden_layers=[64, 128, 64]):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(state_size, hidden_layers[0])])\n",
    "        \n",
    "        A = hidden_layers[:-1]  # -> [64, 128] (using the predefined values)\n",
    "        B = hidden_layers[1:]   # -> [128, 64]\n",
    "        in_out = zip(A, B)      # -> [(64, 128), (128, 64)]\n",
    "        self.hidden_layers.extend([nn.Linear(a, b) for a, b in in_out])\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_layers[-1], action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        for layer in self.hidden_layers:\n",
    "            state = layer(state)\n",
    "            state = F.relu(state)\n",
    "        state = self.output_layer(state)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 DuelingDQN\n",
    "\n",
    "On the other hand, `DuelingDQN` has two streams of independent layers: the **value** of a state $V(s)$ (scalar) and the **advantage** of taking an action $A(s, a)$ (same shape as the output), related by $A(s, a) = Q(s, a) - V(s)$. The idea is that, in many scenarios, the value of a state won't depend that much on the action the agent takes, so decoupling these values can help make better estimates.\n",
    "\n",
    "So, the class `DuelingDQN` uses two identical streams of independent fully connected layers:\n",
    "\n",
    "- `state_size` $\\to 64$ and `relu`\n",
    "- $64 \\to 128$ and `relu`\n",
    "- $128 \\to 64$ and `relu`\n",
    "\n",
    "The streams end in:\n",
    "\n",
    "- $64 \\to A(s, a)$, that is $64 \\to$ `action_size` and no activation\n",
    "- $64 \\to V(s)$, that is $64 \\to 1$ and no activation\n",
    "\n",
    "To merge them, however, we can't simply add $V$ and $A$ as $Q(s, a) = V(s) + A(s, a)$, because $A$ and $V$ **won't be identifiable**. We can, instead, write\n",
    "\n",
    "$$Q(s, a) = V(s) + A(s, a) - \\max_{a'} A(S, a')$$\n",
    "\n",
    "so that when the best action $a^*$ is selected, we get that $Q(s, a^*) = V(s)$, something that is not guaranteed otherwise. To further improve this, we can substitute the $\\max$ operator for a mean of the available actions. This is the final equation used in the `DuelingDQN` class, where $|\\mathbb{A}(s)| =$ `action_size`.\n",
    "\n",
    "$$Q(s, a) = V(s) + A(s, a) - \\frac{1}{|\\mathbb{A}(s)|} \\sum_{a' \\in \\mathbb{A}(s)} A(s, a')$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Error estimation\n",
    "\n",
    "Since this is a bootstrapping method, when we're calculating the **temporal-difference** error $\\delta_t$ we'll have to rely on an estimate of future returns. Two proposed variants for this are fixed Q-targets and Double DQN.\n",
    "\n",
    "The functions `dt_dqn`, `dt_double_dqn` implement these two variants. Both take an extra boolean parameter `d` that signals when the episode is over, since in that case, there is no future return and the TD error is simply $\\delta_t = r - Q(s, a)$.\n",
    "\n",
    "Note that these functions expect `s`, `a`, `r`, `ns` and `d` as **torch tensors**, where the number of rows indicates the batch size. The return value is also a vector with the same number of rows, one for each tuple.\n",
    "\n",
    "#### 4.1 Fixed Q-targets\n",
    "\n",
    "The original paper proposed the method of fixed Q-targets, essentially **duplicating the network**, creating $Q_{local}$ and $Q_{target}$, where the former is used to determine the policy and the latter is updated less frequently and only used to estimate future returns. In particular, the equation for $\\delta_t$ is\n",
    "\n",
    "$$\\delta_t(s, a, r, s') = r + \\gamma \\max_{a'} Q_{target}(s', a') - Q_{local}(s, a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt_dqn(s, a, r, ns, d, q_local, q_target, gamma):\n",
    "    with torch.no_grad():   # no need for gradients when we're evaluating the TD target\n",
    "        QT = q_target(ns)   # evaluate the next state using the target network (out: [n * action_size])\n",
    "        QT = QT.max(1)      # take the max along the column (actions) (out: two tensors [n])\n",
    "        QT = QT[0]          # - [0] has the max values for each element in the batch, \n",
    "                            # - [1] has the indexes of the max values\n",
    "\n",
    "    a = a.unsqueeze(1)      # reshape [n] -> [n * 1]\n",
    "    QL = q_local(s)         # evaluate the current state using the local network (out: [n * action_size])\n",
    "    QL = QL.gather(1, a)    # for each row, take the column in QL indicated by a (out: [n * 1])\n",
    "    QL = QL.squeeze(1)      # reshape [n * 1] -> [n]\n",
    "\n",
    "    return r + gamma * QT * (1 - torch.FloatTensor(d)) - QL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Double DQN\n",
    "\n",
    "This is largely based on fixed Q-targets, since it also uses $Q_{local}$ and $Q_{target}$. We can rewrite $\\max Q$ using $\\arg \\max$ to break down the task of **choosing** and **evaluating** an action in two different steps.\n",
    "\n",
    "$$\\max_{a'} Q_{target}(s', a') = Q_{target}(s', \\arg \\max_{a'} Q_{target}(s', a'))$$\n",
    "\n",
    "This can lead to the **overestimation** of targets. Simply put, when taking the $\\max$ among noisy numbers (which is especially true in the beginning) we're likely to pick the action where the approximator adds more positive noise. Separating the task of picking and evaluating the action to two different approximators may decrease this issue, since both networks now have to \"agree\" on the outcome of an action.\n",
    "\n",
    "The original estimate can be improved using the one provided in the Double DQN paper. The proposed solution is to use $Q_{local}$ for the choice and $Q_{target}$ for the evaluation as follows\n",
    "\n",
    "$$\n",
    "Q_{target}(s', \\arg \\max_{a'} Q_{local}(s', a'))\\\\\n",
    "\\implies \\delta_t(s, a, r, s') = r + \\gamma Q_{target}(s', \\arg \\max_{a'} Q_{local}(s', a')) - Q_{local}(s, a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt_double_dqn(s, a, r, ns, d, q_local, q_target, gamma):\n",
    "    with torch.no_grad():         # no need for gradients when we're evaluating the TD target\n",
    "        QLns = q_local(ns)        # evaluate the next state using the local network (out: [n * action_size])\n",
    "        QLns = QLns.max(1)        # take the max along the column (actions) (out: two tensors [n])\n",
    "        QLns = QLns[1]            # [1] has the indexes of the max values\n",
    "        QLns = QLns.unsqueeze(1)  # reshape [n] -> [n * 1]\n",
    "\n",
    "        QT = q_target(ns)         # evaluate the next state using the target network (out: [n * action_size])\n",
    "        QT = QT.gather(1, QLns)   # for each row, take the value estimated by the target network for\n",
    "                                  # the best action estimated by the local network (out: [n * 1])\n",
    "        QT = QT.squeeze(1)        # reshape [n * 1] -> [n]\n",
    "\n",
    "    a = a.unsqueeze(1)            # reshape [n] -> [n * 1]\n",
    "    QL = q_local(s)               # evaluate the current state using the local network (out: [n * action_size])\n",
    "    QL = QL.gather(1, a)          # for each row, take the column in QL indicated by a (out: [n * 1])\n",
    "    QL = QL.squeeze(1)            # reshape [n * 1] -> [n]\n",
    "\n",
    "    return r + gamma * QT * (1 - torch.FloatTensor(d)) - QL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Agent\n",
    "\n",
    "Let's put the pieces together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetworkAgent():\n",
    "    \n",
    "    def __init__(self, QNetwork, state_size, action_size, \n",
    "                 replay_buffer, Delta, \n",
    "                 eps=1, eps_decay=0.9995, min_eps=0.0001, gamma=0.99, \n",
    "                 alpha=0.001, tau=0.01,\n",
    "                 update_every=4, batch_size=64, learning=True):\n",
    "        self.state_size, self.action_size = state_size, action_size\n",
    "        self.q_local, self.q_target = QNetwork(state_size, action_size), QNetwork(state_size, action_size)\n",
    "        self.q_target.eval()\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.Delta = Delta\n",
    "        self.optimizer = optim.Adam(self.q_local.parameters(), lr=alpha)\n",
    "        self.learning = learning\n",
    "        self.eps, self.eps_decay, self.min_eps = eps, eps_decay, min_eps\n",
    "        self.gamma, self.alpha, self.tau = gamma, alpha, tau\n",
    "        self.update_every, self.update_i, self.batch_size = update_every, 0, batch_size\n",
    "        \n",
    "    def act(self, s):\n",
    "        if not self.learning or np.random.uniform() > self.eps:\n",
    "            with torch.no_grad():\n",
    "                s = torch.FloatTensor(s).unsqueeze(0)\n",
    "                return int(self.q_local(s).max(1)[1])\n",
    "        else:\n",
    "            return np.random.randint(self.action_size)\n",
    "    \n",
    "    def store(self, s, a, r, ns, d):\n",
    "        p = self.replay_buffer.max_priority()\n",
    "        self.replay_buffer.add((s, a, r, ns, d, p))\n",
    "        if self.update_i == 0 and self.replay_buffer.size() >= self.batch_size:\n",
    "            self.learn()\n",
    "        self.update_i = (self.update_i + 1) % self.update_every\n",
    "        self.eps = max(self.eps * self.eps_decay, self.min_eps)\n",
    "    \n",
    "    def learn(self):\n",
    "        s, a, r, ns, d, w = self.replay_buffer.sample(self.batch_size)\n",
    "        td_delta = self.Delta(s, a, r, ns, d, self.q_local, self.q_target, self.gamma)\n",
    "        self.replay_buffer.update_priority(zip(s, a, r, ns, d, torch.abs(td_delta)))\n",
    "                \n",
    "        self.optimizer.zero_grad()\n",
    "        loss = torch.sum(w * (td_delta ** 2))  # weighted mse\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for local, target in zip(self.q_local.parameters(), self.q_target.parameters()):\n",
    "                target.copy_(target + self.tau * (local - target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QNetworkAgent(\n",
    "    DQN,\n",
    "    state_size, action_size,\n",
    "    UniformReplayBuffer(100_000),\n",
    "    dt_double_dqn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Episode: 1000, Score: 11.0, Average: 13.11"
     ]
    }
   ],
   "source": [
    "# last100 = []\n",
    "for i in range(355, 1001):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = env_info.vector_observations[0]\n",
    "    score = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "\n",
    "        env_info = env.step(action)[brain_name]\n",
    "        next_state = env_info.vector_observations[0]\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "\n",
    "        agent.store(state, action, reward, next_state, done)\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "    # if i % 10 == 0:\n",
    "    \n",
    "    last100.append(score)\n",
    "    if len(last100) > 100:\n",
    "        last100.pop(0)\n",
    "    \n",
    "    average = np.mean(last100)\n",
    "    print(\"\\rEpisode: {}, Score: {}, Average: {}\".format(i, score, average), end=\"\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    \"\"\"\n",
    "    if average > 13:\n",
    "        print(\"\\rSolved in {} episodes. Average: {}\".format(i, average), end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        break\n",
    "    \"\"\"\n",
    "    \n",
    "# print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(agent.q_local.state_dict(), 'checkpoint.pth')\n",
    "# agent.q_local.load_state_dict(torch.load('checkpoint.pth', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "13.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "agent.learning = False\n",
    "state = env_info.vector_observations[0]\n",
    "score = 0\n",
    "done = False\n",
    "while not done:\n",
    "    action = agent.act(state)\n",
    "\n",
    "    env_info = env.step(action)[brain_name]\n",
    "    next_state = env_info.vector_observations[0]\n",
    "    reward = env_info.rewards[0]\n",
    "    done = env_info.local_done[0]\n",
    "\n",
    "    agent.store(state, action, reward, next_state, done)\n",
    "    score += reward\n",
    "    state = next_state\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p1_navigation",
   "language": "python",
   "name": "p1_navigation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}